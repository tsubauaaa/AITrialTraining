{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AITraining3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNKn66T2P9E2wt/kkiX6l27",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsubauaaa/AITrialTraining/blob/main/Training3/AITraining3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEvi8gKZ51qO",
        "outputId": "2230b4c8-2357-4829-e5f3-8d766ab4d3e3"
      },
      "source": [
        " # Google Driveをマウントする\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_0hFdJwXF7w",
        "outputId": "90e7b1cd-0e3b-4be4-a4c5-102e396146d2"
      },
      "source": [
        "# install MeCab\n",
        "!apt-get -q -y install sudo file mecab libmecab-dev mecab-ipadic-utf8 git curl python-mecab > /dev/null\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git > /dev/null \n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n > /dev/null 2>&1\n",
        "!pip install mecab-python3 > /dev/null\n",
        "# check path to \"ipadic-neologd\"\n",
        "!echo `mecab-config --dicdir`\"/mecab-ipadic-neologd\"\n",
        "\n",
        "!ln -s /etc/mecabrc /usr/local/etc/mecabrc"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'mecab-ipadic-neologd' already exists and is not an empty directory.\n",
            "/usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd\n",
            "ln: failed to create symbolic link '/usr/local/etc/mecabrc': File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1VKEi7usaW2"
      },
      "source": [
        "import json\n",
        "import gensim\n",
        "import MeCab\n",
        "import pandas as pd\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOnsQQhr5pky"
      },
      "source": [
        "decoder = json.JSONDecoder()\n",
        "all_datasets_list = []\n",
        "with open('/content/drive/My Drive/Colab Notebooks/AITraining/dataset_ja_dev.json') as f:\n",
        "    line = f.readline()\n",
        "    while line:\n",
        "        all_datasets_list.append(decoder.raw_decode(line)[0])\n",
        "        line = f.readline()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFfnpT1G6rpW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3a3ccdb-87d8-492a-9046-f93c954acfd7"
      },
      "source": [
        "all_datasets_list[1]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'language': 'ja',\n",
              " 'product_category': 'wireless',\n",
              " 'product_id': 'product_ja_0821731',\n",
              " 'review_body': 'ホームボタン周りの気泡が全く抜けません。 返金をお願いしましたが、断られた。',\n",
              " 'review_id': 'ja_0944897',\n",
              " 'review_title': '欠陥品',\n",
              " 'reviewer_id': 'reviewer_ja_0192786',\n",
              " 'stars': '1'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "id": "04UlblNH7dBK",
        "outputId": "5f5f225b-de28-4d53-f8df-90a3b39f2e61"
      },
      "source": [
        "datasets_list = []\n",
        "for data in all_datasets_list:\n",
        "    review_body = data['review_body']\n",
        "    stars = data['stars']\n",
        "    datasets_list.append([review_body, stars])\n",
        "datasets = pd.DataFrame(datasets_list, columns = ['review_body' , 'stars'])\n",
        "datasets"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>review_body</th>\n",
              "      <th>stars</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>味自体及び吸い心地は良いのだが、不良品が多過ぎる。私の場合５本のうち２本が蒸気も出ず、吸い込...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ホームボタン周りの気泡が全く抜けません。 返金をお願いしましたが、断られた。</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>新旧含めて4つのカーテンレールがあるのですが、使用出来るカーテンレールはありませんでした。 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>予約注文でしたが、どこから特典であるpdfダウンロードすればよいのでしょうか…</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>前のレビューにもありましたが、片方が全く動きません。 返品しようにも、なんだかめんどくさいし...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4995</th>\n",
              "      <td>ミニオンが好きで、息子に買いました。 親子で楽しく遊んでます。</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4996</th>\n",
              "      <td>まずレーザーの光が強いw 昔 ゲーセンで取ったヤツの3倍くらい 暗闇でレーザーの光が当たった...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4997</th>\n",
              "      <td>色もち、発色もよく、ティントによくある\"激しい唇の荒れ\"が少ないのでとても使いやすいなと思い...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4998</th>\n",
              "      <td>1年前に別メーカーのバッテリーを交換して、使えましたが、スマホ確認のところで認識さらませんで...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4999</th>\n",
              "      <td>前なら剃った次の日はまた、ポツポツでそれを抜いてましたがポツポツがありません！</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            review_body stars\n",
              "0     味自体及び吸い心地は良いのだが、不良品が多過ぎる。私の場合５本のうち２本が蒸気も出ず、吸い込...     1\n",
              "1                ホームボタン周りの気泡が全く抜けません。 返金をお願いしましたが、断られた。     1\n",
              "2     新旧含めて4つのカーテンレールがあるのですが、使用出来るカーテンレールはありませんでした。 ...     1\n",
              "3               予約注文でしたが、どこから特典であるpdfダウンロードすればよいのでしょうか…     1\n",
              "4     前のレビューにもありましたが、片方が全く動きません。 返品しようにも、なんだかめんどくさいし...     1\n",
              "...                                                 ...   ...\n",
              "4995                    ミニオンが好きで、息子に買いました。 親子で楽しく遊んでます。     5\n",
              "4996  まずレーザーの光が強いw 昔 ゲーセンで取ったヤツの3倍くらい 暗闇でレーザーの光が当たった...     5\n",
              "4997  色もち、発色もよく、ティントによくある\"激しい唇の荒れ\"が少ないのでとても使いやすいなと思い...     5\n",
              "4998  1年前に別メーカーのバッテリーを交換して、使えましたが、スマホ確認のところで認識さらませんで...     5\n",
              "4999            前なら剃った次の日はまた、ポツポツでそれを抜いてましたがポツポツがありません！     5\n",
              "\n",
              "[5000 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sC1UjH2W6yge"
      },
      "source": [
        "tagger = MeCab.Tagger(\"-Owakati\")\n",
        "# param: dataset_list[]['review_body']\n",
        "# return: 分かちした単語のリスト\n",
        "def make_wakati(sentence):\n",
        "    # MeCabで分かち書き\n",
        "    sentence = tagger.parse(sentence)\n",
        "    # 半角全角英数字除去\n",
        "    sentence = re.sub(r'[0-9０-９a-zA-Zａ-ｚＡ-Ｚ]+', \" \", sentence)\n",
        "    # 記号もろもろ除去\n",
        "    sentence = re.sub(r'[\\．_－―─！＠＃＄％＾＆\\-‐|\\\\＊\\“（）＿■×+α※÷⇒—●★☆〇◎◆▼◇△□(：〜～＋=)／*&^%$#@!~`){}［］…\\[\\]\\\"\\'\\”\\’:;<>?＜＞〔〕〈〉？、。・,\\./『』【】「」→←○《》≪≫\\n\\u3000]+', \"\", sentence)\n",
        "    # スペースで区切って形態素の配列へ\n",
        "    wakati = sentence.split(\" \")\n",
        "    # 空の要素は削除\n",
        "    wakati = list(filter((\"\").__ne__, wakati))\n",
        "    return wakati"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAOqxOpxhe6s",
        "outputId": "452be270-1016-47da-c3b5-ab616912636f"
      },
      "source": [
        "# w2vする\n",
        "w2v_model = gensim.models.Word2Vec.load('/content/drive/My Drive/Colab Notebooks/AITraining/w2v/w2v.model')\n",
        "word2vec = {}\n",
        "no_words = []\n",
        "for dataset in datasets_list:\n",
        "    wakati = make_wakati(dataset[0])\n",
        "    for word in wakati:\n",
        "        if word in word2vec:\n",
        "            continue\n",
        "        if word not in list(w2v_model.wv.vocab):\n",
        "            no_words.append(word)\n",
        "            continue\n",
        "        word2vec[word] = w2v_model.wv[word]\n",
        "print(len(word2vec))\n",
        "print(len(no_words))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "12739\n",
            "711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-Q4mAMD1xHs"
      },
      "source": [
        "# 単語をベクトルデータに変換\n",
        "# PyTorchのLSTMのインプットになるデータなので、もちろんtensor型で\n",
        "def sentence2vec(sentence):\n",
        "    wakati = make_wakati(sentence)\n",
        "    # return torch.tensor([word2vec[w] for w in wakati])\n",
        "    return [word2vec[w] for w in wakati]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "egd-cKlV12PD"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# nn.Moduleを継承して新しいクラスを作る。決まり文句\n",
        "class LSTMRegressor(nn.Module):\n",
        "    # モデルで使う各ネットワークをコンストラクタで定義\n",
        "    def __init__(self, embedding_dim, hidden_dim):\n",
        "        # 親クラスのコンストラクタ。決まり文句\n",
        "        super(LSTMRegressor, self).__init__()\n",
        "        # 隠れ層の次元数。これは好きな値に設定しても行列計算の過程で出力には出てこないので。\n",
        "        self.hidden_dim = hidden_dim\n",
        "        # LSTMの隠れ層。これ１つでOK。超便利。\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        # LSTMの出力を受け取って全結合してsoftmaxに食わせるための１層のネットワーク\n",
        "        self.out = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    # 順伝播処理はforward関数に記載\n",
        "    def forward(self, x, h, c):\n",
        "        # 2次元テンソルをLSTMに食わせられる様にviewで３次元テンソルにした上でLSTMへ流す。\n",
        "        # 上記で説明した様にmany to oneのタスクを解きたいので、第二戻り値だけ使う。\n",
        "        _, (h, c) = self.lstm(x, (h, c))\n",
        "        # hは３次元テンソルになってしまっているので2次元に調整して全結合。\n",
        "        output = self.out(h.view(-1, self.hidden_dim))\n",
        "        return output\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        h = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        c = torch.zeros(1, batch_size, self.hidden_dim)\n",
        "        return h, c"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ_DZmzN2Lac"
      },
      "source": [
        "# EMBEDDING_DIM = 200\n",
        "# HIDDEN_DIM = 128\n",
        "# lstm = LSTMRegressor(EMBEDDING_DIM, HIDDEN_DIM)\n",
        "# s1 = datasets_list[0][0]\n",
        "# s2 = datasets_list[0][1]\n",
        "# print(s1)\n",
        "# # 味自体及び吸い心地は良いのだが、不良品が多過ぎる。私の場合５本のうち２本が蒸気も出ず、吸い込み も出来なかった。腹が立ってごみ箱行きでした。こんなものは２度と購入する気はない。 返品するのも交渉するのも、金額も金額だからと面倒くさがってしない方が多いのではないか？ 最初から不良品多しとでも表記しておいたら如何？\n",
        "# print(make_wakati(s1))\n",
        "# # ['味', '自体', '及び', '吸い', '心地', 'は', '良い', 'の', 'だ', 'が', '不', '良品', 'が', '多', '過ぎる', '私', 'の', '場合', '本', 'の', 'うち', '本', 'が', '蒸気', 'も', '出', 'ず', '吸い込み', 'も', '出来', 'なかっ', 'た', '腹', 'が', '立っ', 'て', 'ごみ箱', '行き', 'でし', 'た', 'こんな', 'もの', 'は', '度', 'と', '購入', 'する', '気', 'は', 'ない', '返品', 'する', 'の', 'も', '交渉', 'する', 'の', 'も', '金額', 'も', '金額', 'だ', 'から', 'と', '面倒く', 'さ', 'がっ', 'て', 'し', 'ない', '方', 'が', '多い', 'の', 'で', 'は', 'ない', 'か', '最初', 'から', '不', '良品', '多し', 'と', 'でも', '表記', 'し', 'て', 'おい', 'たら', '如何']\n",
        "# inputs1 = sentence2vec(s1)\n",
        "# inputs1 = torch.tensor(inputs1)\n",
        "# print(inputs1)\n",
        "# print(inputs1.shape)\n",
        "# out = lstm(inputs1.view(len(inputs1), 1, -1), h, c)\n",
        "# print(out)\n",
        "# print(s2)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv1lVDDt38wy",
        "outputId": "837ad0ae-2ee5-469e-b06a-d515d4cf7b84"
      },
      "source": [
        "len_datasets = len(datasets_list)\n",
        "# starsをtensorにする\n",
        "category2index = {}\n",
        "for i in range(len_datasets):\n",
        "    star = datasets_list[i][1]\n",
        "    if star in category2index: continue\n",
        "    category2index[star] = len(category2index)\n",
        "print(category2index)\n",
        "\n",
        "def category2tensor(star):\n",
        "    # return torch.tensor([category2index[star]], dtype=torch.float)\n",
        "    return category2index[star]\n",
        "\n",
        "print(category2tensor(\"2\"))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'1': 0, '2': 1, '3': 2, '4': 3, '5': 4}\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRUjm79i48M6"
      },
      "source": [
        "# 生データのデータセットクラス\n",
        "# TODO: ここでdataをsentence2vecしたほうが◎\n",
        "class ReviewRawDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, datasets_list, transform = None):\n",
        "        self.transform = transform\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "        self.skips = 0\n",
        "        for x in datasets_list:\n",
        "            try:\n",
        "                self.data.append(sentence2vec(x[0]))\n",
        "                self.label.append(category2tensor(x[1]))\n",
        "            except KeyError:\n",
        "                # print(f'keyError occured: {x}')\n",
        "                self.skips += 1\n",
        "                continue\n",
        "        print(f\"skip count: {self.skips}\")\n",
        "        self.datanum = len(self.label)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.datanum\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        out_label = self.label[idx]\n",
        "        out_data = self.data[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            out_data = self.transform(out_data)\n",
        "\n",
        "        return out_data, out_label"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R_dFxaOs2rWA"
      },
      "source": [
        "# (最大シーケンス長×バッチサイズ×特徴量次元数)のTensorを適当にゼロ埋めして返す\n",
        "class PadCollate:\n",
        "    \"\"\"\n",
        "    a variant of callate_fn that pads according to the longest sequence in\n",
        "    a batch of sequences\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim=1):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            dim - the dimension to be padded (dimension of time in sequences)\n",
        "        \"\"\"\n",
        "        self.dim = dim\n",
        "\n",
        "    def pad_collate(self, batch):\n",
        "        '''\n",
        "        Padds batch of variable length\n",
        "\n",
        "        note: it converts things ToTensor manually here since the ToTensor transform\n",
        "        assume it takes in images rather than arbitrary tensors.\n",
        "        '''\n",
        "        ## get sequence lengths\n",
        "        lengths = torch.tensor([len(review) for review, star in batch])\n",
        "        ## padd\n",
        "        batch_review = [torch.Tensor(review) for review, star in batch]\n",
        "        batch_review = torch.nn.utils.rnn.pad_sequence(batch_review, batch_first=True)\n",
        "        star = torch.tensor([star for review, star in batch], dtype=torch.float)\n",
        "        return batch_review, star\n",
        "\n",
        "    def __call__(self, batch):\n",
        "        return self.pad_collate(batch)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzcbWAzQzRDh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1777e3f-6c66-401c-96bf-72a90f21e973"
      },
      "source": [
        "print(f\"datasets_list length: {len(datasets_list)}\")\n",
        "review_dataset = ReviewRawDataset(datasets_list)\n",
        "print(f\"review_dataset length: {len(review_dataset)}\")\n",
        "n_samples = len(review_dataset)\n",
        "train_size = int(len(review_dataset) * 0.8)\n",
        "test_size = n_samples - train_size\n",
        "train_dataset, test_dataset = torch.utils.data.random_split(review_dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"train_dataset length: {len(train_dataset)}\")\n",
        "print(f\"test_dataset length: {len(test_dataset)}\")\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=PadCollate())\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, collate_fn=PadCollate())\n",
        "print(f\"train_loader length: {len(train_loader)}\")\n",
        "print(f\"test_loader length: {len(test_loader)}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "datasets_list length: 5000\n",
            "skip count: 596\n",
            "review_dataset length: 4404\n",
            "train_dataset length: 3523\n",
            "test_dataset length: 881\n",
            "train_loader length: 111\n",
            "test_loader length: 881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTiCbpcY3X-e",
        "outputId": "a20f16f7-58fe-411d-ab34-c788a44768d7"
      },
      "source": [
        "train_dataiter = iter(train_loader)\n",
        "# データ取得\n",
        "data, target = train_dataiter.next()\n",
        "print(data.shape, target.shape) # data: (batch_size, seq_len), target: (batch_size)\n",
        "print(data.dtype, target.dtype) # data: (batch_size, seq_len), target: (batch_size)\n",
        "print(data, target) # data: (batch_size, seq_len), target: (batch_size)\n",
        "print(data[0].sum())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([32, 276, 200]) torch.Size([32])\n",
            "torch.float32 torch.float32\n",
            "tensor([[[ 1.7451e+00, -1.3074e-01,  1.5752e+00,  ..., -2.0835e+00,\n",
            "          -1.1018e+00,  4.5962e-01],\n",
            "         [ 2.5150e+00, -1.3348e-01,  1.8852e+00,  ..., -3.7523e+00,\n",
            "           4.4167e-02,  9.3814e-01],\n",
            "         [ 1.6621e+00,  1.0172e+00,  3.1337e+00,  ..., -1.0852e+00,\n",
            "          -1.5092e+00, -4.7079e-02],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[-2.2571e+00, -3.4708e-01,  2.6206e-01,  ...,  1.5691e+00,\n",
            "           1.1910e-03, -1.7663e+00],\n",
            "         [-3.3881e+00,  1.5009e+00, -1.8426e+00,  ..., -2.5546e+00,\n",
            "          -6.3243e-01,  1.1873e+00],\n",
            "         [-5.4255e-02, -1.2595e+00,  2.4858e-01,  ..., -3.3852e-01,\n",
            "          -2.4594e+00, -1.2441e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 2.5355e+00, -4.6035e+00,  4.3536e-01,  ...,  8.2628e-01,\n",
            "           7.9860e-01,  2.7706e+00],\n",
            "         [-1.8343e+00, -1.7475e+00, -1.5776e-02,  ..., -2.5781e-01,\n",
            "           2.1230e+00,  1.3894e+00],\n",
            "         [-5.8987e-01, -1.6077e+00, -1.0455e+00,  ..., -3.0951e+00,\n",
            "          -1.3661e+00,  1.5654e+00],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[-7.3906e-02, -6.1263e-01,  7.2701e-01,  ...,  1.5322e+00,\n",
            "          -2.7001e-02, -3.2689e-01],\n",
            "         [ 1.2319e-01, -1.9883e+00,  2.8928e-01,  ...,  1.5262e+00,\n",
            "          -1.9099e+00,  9.0037e-01],\n",
            "         [-1.7337e-01,  1.5519e+00, -2.1845e+00,  ...,  1.8394e+00,\n",
            "          -1.3842e+00, -3.0742e+00],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 5.0339e-01,  1.7972e+00,  1.9045e+00,  ...,  2.1075e-01,\n",
            "          -1.7776e+00, -1.0866e+00],\n",
            "         [-1.0880e+00, -6.2813e-01, -2.0233e+00,  ...,  6.3316e-01,\n",
            "          -8.4975e-01, -2.5505e+00],\n",
            "         [-3.3521e-01, -1.0473e+00,  1.9284e+00,  ...,  1.2406e+00,\n",
            "          -4.7001e-02,  7.3982e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]],\n",
            "\n",
            "        [[ 6.8294e-01, -9.0090e-01,  1.7613e+00,  ...,  5.9837e+00,\n",
            "           3.1404e-01,  3.6090e-01],\n",
            "         [-7.5911e-01,  1.4962e+00,  5.6078e-02,  ..., -1.2164e-01,\n",
            "          -1.2029e+00, -1.5203e+00],\n",
            "         [-1.0601e+00,  9.6321e-01,  1.2834e+00,  ..., -7.5349e-01,\n",
            "           5.3040e-01, -2.3979e-01],\n",
            "         ...,\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00],\n",
            "         [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
            "           0.0000e+00,  0.0000e+00]]]) tensor([4., 3., 3., 0., 4., 4., 0., 0., 3., 3., 4., 3., 4., 4., 4., 0., 1., 1.,\n",
            "        2., 0., 0., 4., 3., 0., 4., 1., 0., 3., 2., 3., 4., 2.])\n",
            "tensor(14.1861)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsoIFDE06TFP",
        "outputId": "f26d7ed5-e0fe-416f-c529-c6d3f601a19c"
      },
      "source": [
        "import torch.optim as optim\n",
        "# 単語のベクトル次元数\n",
        "EMBEDDING_DIM = 200\n",
        "# 隠れ層の次元数\n",
        "HIDDEN_DIM = 128\n",
        "# モデル宣言\n",
        "model = LSTMRegressor(EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "# 損失関数はNLLLoss()を使う。LogSoftmaxを使う時はこれを使うらしい。\n",
        "loss_function = nn.MSELoss()\n",
        "# 最適化の手法はSGDで。lossの減りに時間かかるけど、一旦はこれを使う。\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# 各エポックの合計loss値を格納する\n",
        "losses = []\n",
        "for epoch in range(200):\n",
        "    all_loss = 0\n",
        "    for batch_index, (review, star) in enumerate(train_loader):\n",
        "        # モデルが持ってる勾配の情報をリセット\n",
        "        model.zero_grad()\n",
        "        inputs = review.to(device)\n",
        "        # (seq_len, batch_size, feature_size)に並べ替えして入力\n",
        "        inputs = inputs.permute(1, 0, 2)\n",
        "        h, c = model.init_hidden(review.size(0))\n",
        "        h, c = h.to(device), c.to(device)\n",
        "        out = model(inputs, h, c)\n",
        "        # (batch_size, 1) -> (batch_size)にsqueeze\n",
        "        out = torch.squeeze(out)\n",
        "        target = star.to(device)\n",
        "        # 正解とのlossを計算\n",
        "        loss = loss_function(out, target)\n",
        "        # 勾配をセット\n",
        "        loss.backward()\n",
        "        # 逆伝播でパラメータ更新\n",
        "        optimizer.step()\n",
        "        # lossを集計\n",
        "        all_loss += loss.item()\n",
        "    losses.append(all_loss)\n",
        "    print(\"epoch\", epoch, \"\\t\" , \"loss\", all_loss / (batch_index + 1))\n",
        "print(\"done.\")"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 \t loss 2.488485510821815\n",
            "epoch 1 \t loss 2.0462449563516154\n",
            "epoch 2 \t loss 2.0618320059131934\n",
            "epoch 3 \t loss 2.0325819404275567\n",
            "epoch 4 \t loss 2.0412022176089586\n",
            "epoch 5 \t loss 2.031755030692161\n",
            "epoch 6 \t loss 2.009174038161029\n",
            "epoch 7 \t loss 2.010254849184741\n",
            "epoch 8 \t loss 1.9793981564407412\n",
            "epoch 9 \t loss 2.0014914327913575\n",
            "epoch 10 \t loss 1.997085250175751\n",
            "epoch 11 \t loss 2.0176818091590127\n",
            "epoch 12 \t loss 2.0022749696765936\n",
            "epoch 13 \t loss 2.0104768018464783\n",
            "epoch 14 \t loss 2.007979612092714\n",
            "epoch 15 \t loss 2.008395483901909\n",
            "epoch 16 \t loss 2.002003592413825\n",
            "epoch 17 \t loss 1.9942161779145937\n",
            "epoch 18 \t loss 1.9967693090438843\n",
            "epoch 19 \t loss 1.9851991534233093\n",
            "epoch 20 \t loss 2.0176796752053336\n",
            "epoch 21 \t loss 2.0067720960926367\n",
            "epoch 22 \t loss 1.9967560188190356\n",
            "epoch 23 \t loss 1.9994155512199745\n",
            "epoch 24 \t loss 2.0003269803416623\n",
            "epoch 25 \t loss 2.0053582782143944\n",
            "epoch 26 \t loss 1.9984173785458814\n",
            "epoch 27 \t loss 1.9971694162300042\n",
            "epoch 28 \t loss 1.9756423637673661\n",
            "epoch 29 \t loss 1.9670186117962674\n",
            "epoch 30 \t loss 1.9716578743479274\n",
            "epoch 31 \t loss 1.8581818481823345\n",
            "epoch 32 \t loss 1.8037661512692769\n",
            "epoch 33 \t loss 1.7926205061040483\n",
            "epoch 34 \t loss 1.9512472507115957\n",
            "epoch 35 \t loss 1.9976072536932457\n",
            "epoch 36 \t loss 1.982657399263468\n",
            "epoch 37 \t loss 1.9958676905245394\n",
            "epoch 38 \t loss 1.9836057693034679\n",
            "epoch 39 \t loss 1.9845864214338698\n",
            "epoch 40 \t loss 1.9953992506405254\n",
            "epoch 41 \t loss 1.977191696295867\n",
            "epoch 42 \t loss 1.971660412915118\n",
            "epoch 43 \t loss 1.9807023776544106\n",
            "epoch 44 \t loss 1.9883403080003756\n",
            "epoch 45 \t loss 2.0255869197415874\n",
            "epoch 46 \t loss 2.0059929130313634\n",
            "epoch 47 \t loss 1.9889605732651445\n",
            "epoch 48 \t loss 1.9910579049909436\n",
            "epoch 49 \t loss 1.9820305001628291\n",
            "epoch 50 \t loss 2.0010789042120583\n",
            "epoch 51 \t loss 1.99884667482462\n",
            "epoch 52 \t loss 1.9795450519871067\n",
            "epoch 53 \t loss 1.963695166884242\n",
            "epoch 54 \t loss 1.8136978503820058\n",
            "epoch 55 \t loss 1.420506135300473\n",
            "epoch 56 \t loss 1.1884308398306906\n",
            "epoch 57 \t loss 0.9634066967813818\n",
            "epoch 58 \t loss 0.795413987996342\n",
            "epoch 59 \t loss 0.6205927026164424\n",
            "epoch 60 \t loss 0.49258702389291814\n",
            "epoch 61 \t loss 0.4103488680478689\n",
            "epoch 62 \t loss 0.3406440540476962\n",
            "epoch 63 \t loss 0.296490462647902\n",
            "epoch 64 \t loss 0.24430308887013444\n",
            "epoch 65 \t loss 0.20645535119750477\n",
            "epoch 66 \t loss 0.16841011135293557\n",
            "epoch 67 \t loss 0.14442163373570185\n",
            "epoch 68 \t loss 0.136521843840947\n",
            "epoch 69 \t loss 0.12338181152134328\n",
            "epoch 70 \t loss 0.10345135420258786\n",
            "epoch 71 \t loss 0.09076769836246967\n",
            "epoch 72 \t loss 0.11872462787338205\n",
            "epoch 73 \t loss 0.11541949571655677\n",
            "epoch 74 \t loss 0.11873907085743036\n",
            "epoch 75 \t loss 0.10607202530645572\n",
            "epoch 76 \t loss 0.09034917020314448\n",
            "epoch 77 \t loss 0.08354335523269198\n",
            "epoch 78 \t loss 0.10752091179224285\n",
            "epoch 79 \t loss 0.09700807350108752\n",
            "epoch 80 \t loss 0.08216197370878749\n",
            "epoch 81 \t loss 0.08369251557097242\n",
            "epoch 82 \t loss 0.06139252329798969\n",
            "epoch 83 \t loss 0.057024626086491184\n",
            "epoch 84 \t loss 0.048083271067757324\n",
            "epoch 85 \t loss 0.059227156838788104\n",
            "epoch 86 \t loss 0.05027891790309736\n",
            "epoch 87 \t loss 0.038799021330133486\n",
            "epoch 88 \t loss 0.04195347018036488\n",
            "epoch 89 \t loss 0.04120953472096238\n",
            "epoch 90 \t loss 0.03433450093274718\n",
            "epoch 91 \t loss 0.02725121240668536\n",
            "epoch 92 \t loss 0.026231740924387095\n",
            "epoch 93 \t loss 0.04968129181898795\n",
            "epoch 94 \t loss 0.05039425280683481\n",
            "epoch 95 \t loss 0.03749120161608533\n",
            "epoch 96 \t loss 0.0439546907589108\n",
            "epoch 97 \t loss 0.030543936496214556\n",
            "epoch 98 \t loss 0.023795768635364267\n",
            "epoch 99 \t loss 0.022094034124165773\n",
            "epoch 100 \t loss 0.025985287259089516\n",
            "epoch 101 \t loss 0.023230300536630926\n",
            "epoch 102 \t loss 0.022928391314774484\n",
            "epoch 103 \t loss 0.021477301747680786\n",
            "epoch 104 \t loss 0.02120967312959266\n",
            "epoch 105 \t loss 0.028926787820334237\n",
            "epoch 106 \t loss 0.029299517468389897\n",
            "epoch 107 \t loss 0.0377789288171911\n",
            "epoch 108 \t loss 0.028685493581416505\n",
            "epoch 109 \t loss 0.02199806854221131\n",
            "epoch 110 \t loss 0.0670028635560795\n",
            "epoch 111 \t loss 0.054637285796841524\n",
            "epoch 112 \t loss 0.03770252006749312\n",
            "epoch 113 \t loss 0.025824007456534886\n",
            "epoch 114 \t loss 0.02284129160280163\n",
            "epoch 115 \t loss 0.019283726175844267\n",
            "epoch 116 \t loss 0.016537116334966576\n",
            "epoch 117 \t loss 0.017059723299147712\n",
            "epoch 118 \t loss 0.014819484124346092\n",
            "epoch 119 \t loss 0.024183439487289335\n",
            "epoch 120 \t loss 0.018763148890774663\n",
            "epoch 121 \t loss 0.01401551760657615\n",
            "epoch 122 \t loss 0.013087701058720012\n",
            "epoch 123 \t loss 0.02479496973289831\n",
            "epoch 124 \t loss 0.057829039565681875\n",
            "epoch 125 \t loss 0.07779199265883313\n",
            "epoch 126 \t loss 0.08036666019475674\n",
            "epoch 127 \t loss 0.03732583608881042\n",
            "epoch 128 \t loss 0.025195760379738367\n",
            "epoch 129 \t loss 0.020225407933262555\n",
            "epoch 130 \t loss 0.01702770931847595\n",
            "epoch 131 \t loss 0.028349307243007462\n",
            "epoch 132 \t loss 0.028929915518273373\n",
            "epoch 133 \t loss 0.0333385412782632\n",
            "epoch 134 \t loss 0.026330257463898207\n",
            "epoch 135 \t loss 0.017351351514760707\n",
            "epoch 136 \t loss 0.012690980906120024\n",
            "epoch 137 \t loss 0.012892164672758465\n",
            "epoch 138 \t loss 0.012496855547274085\n",
            "epoch 139 \t loss 0.011548140848701535\n",
            "epoch 140 \t loss 0.014411064441659831\n",
            "epoch 141 \t loss 0.016031618238435134\n",
            "epoch 142 \t loss 0.013427645970672907\n",
            "epoch 143 \t loss 0.011001382234950995\n",
            "epoch 144 \t loss 0.013396527247948086\n",
            "epoch 145 \t loss 0.04461953860732752\n",
            "epoch 146 \t loss 0.07776240289614007\n",
            "epoch 147 \t loss 0.09083289844361511\n",
            "epoch 148 \t loss 0.053859011186202906\n",
            "epoch 149 \t loss 0.028128111269325018\n",
            "epoch 150 \t loss 0.019349566908335097\n",
            "epoch 151 \t loss 0.019022422992657904\n",
            "epoch 152 \t loss 0.03877378308407224\n",
            "epoch 153 \t loss 0.05268924031406641\n",
            "epoch 154 \t loss 0.037521413208597955\n",
            "epoch 155 \t loss 0.025692953780578735\n",
            "epoch 156 \t loss 0.018984025686157046\n",
            "epoch 157 \t loss 0.013384225652427287\n",
            "epoch 158 \t loss 0.013844203741944118\n",
            "epoch 159 \t loss 0.022193105249489482\n",
            "epoch 160 \t loss 0.03748271612215552\n",
            "epoch 161 \t loss 0.028012936108384852\n",
            "epoch 162 \t loss 0.0205250759734898\n",
            "epoch 163 \t loss 0.037957265008207376\n",
            "epoch 164 \t loss 0.12187501870783733\n",
            "epoch 165 \t loss 0.32235498759929126\n",
            "epoch 166 \t loss 0.16228704671333502\n",
            "epoch 167 \t loss 0.11597093043697847\n",
            "epoch 168 \t loss 0.09167536096395673\n",
            "epoch 169 \t loss 0.14649021531547513\n",
            "epoch 170 \t loss 0.07989171224537196\n",
            "epoch 171 \t loss 0.06268752837838891\n",
            "epoch 172 \t loss 0.05412002794259974\n",
            "epoch 173 \t loss 0.045558145880145395\n",
            "epoch 174 \t loss 0.04159461870906992\n",
            "epoch 175 \t loss 0.03746765628792681\n",
            "epoch 176 \t loss 0.05677470432695102\n",
            "epoch 177 \t loss 0.06654942231378644\n",
            "epoch 178 \t loss 0.05258602483684684\n",
            "epoch 179 \t loss 0.03658993252729242\n",
            "epoch 180 \t loss 0.0392679182711888\n",
            "epoch 181 \t loss 0.027883934800991335\n",
            "epoch 182 \t loss 0.022696571021869377\n",
            "epoch 183 \t loss 0.021698558038134162\n",
            "epoch 184 \t loss 0.020215201661460572\n",
            "epoch 185 \t loss 0.02247910635327702\n",
            "epoch 186 \t loss 0.028912228024522733\n",
            "epoch 187 \t loss 0.0287335636136164\n",
            "epoch 188 \t loss 0.025189651784629705\n",
            "epoch 189 \t loss 0.03434633293088425\n",
            "epoch 190 \t loss 0.03177677182510059\n",
            "epoch 191 \t loss 0.032992087724106806\n",
            "epoch 192 \t loss 0.030442104026725567\n",
            "epoch 193 \t loss 0.04057171317404724\n",
            "epoch 194 \t loss 0.04431193715028532\n",
            "epoch 195 \t loss 0.03013035321297745\n",
            "epoch 196 \t loss 0.0271429684462009\n",
            "epoch 197 \t loss 0.02549410874321889\n",
            "epoch 198 \t loss 0.02943993926253841\n",
            "epoch 199 \t loss 0.031071834268283333\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "F1OrN0w07AA0",
        "outputId": "ec161ad8-ae8a-42ab-93df-f8aa5d530306"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.plot(losses)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc580ababd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0HAS8SmI5kR",
        "outputId": "6fa1c5b1-3069-4472-e343-b955a5ae42e6"
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# テストデータの母数計算(1500)\n",
        "test_num = len(test_loader)\n",
        "# 正解の件数\n",
        "match = 0\n",
        "# 絶対誤差用\n",
        "y_preds = []\n",
        "y_true = []\n",
        "# 勾配自動計算OFF\n",
        "with torch.no_grad():\n",
        "    for batch_index, (review, star) in enumerate(test_loader):\n",
        "        inputs = review.to(device)\n",
        "        h, c = model.init_hidden(review.size(0))\n",
        "        h, c = h.to(device), c.to(device)\n",
        "        # (seq_len, batch_size, feature_size)に並べ替えして入力\n",
        "        out = model(inputs.permute(1, 0, 2), h, c)\n",
        "        out = torch.squeeze(out)\n",
        "        predict = out.item()\n",
        "        target = int(star)\n",
        "        y_preds.append(predict)\n",
        "        y_true.append(target)\n",
        "\n",
        "        # predictを四捨五入して一致数を数える\n",
        "        rounded_pred = round(predict)\n",
        "        if rounded_pred < 0:\n",
        "            rounded_pred = 0\n",
        "        elif rounded_pred > 4:\n",
        "            rounded_pred = 4\n",
        "        # print(f\"pred: {predict}, rounded pred: {rounded_pred}, target: {target}\")\n",
        "        if rounded_pred == target:\n",
        "            match += 1\n",
        "mae = mean_absolute_error(y_true, y_preds)\n",
        "print(f\"Correct answer rate: {match / test_num}, match: {match}, tested_num: {test_num}\")\n",
        "print(f\"Mean Absolute Error: {mae}\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Correct answer rate: 0.358683314415437, match: 316, tested_num: 881\n",
            "Mean Absolute Error: 0.9054456292382028\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X0ONhxHb6K0b"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}